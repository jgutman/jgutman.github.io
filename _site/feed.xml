<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jacqueline Gutman</title>
    <description>Jacqueline Gutman, Center for Data Science. Personal site. Educational data mining.</description>
    <link>https://jgutman.github.io/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 02 Feb 2016 12:29:40 -0500</pubDate>
    <lastBuildDate>Tue, 02 Feb 2016 12:29:40 -0500</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>Audience design preferences in sentence disambiguation</title>
        <description>&lt;p.small&gt;I conducted an eye-tracking study to determine if speakers describing a scene are more likely to explicitly disambiguate sentence structures that caused measurable difficulty for them during comprehension. Participants were first presented with a visual scene and corresponding ambiguous scene description, and eye-tracking trajectories were used as a proxy measure of parsing difficulty. We then asked whether the level of difficulty experienced as a comprehender predicted the probability of providing explicit disambiguation or ambiguity avoidance when describing the visual scene to another individual. We also investigated whether this audience design effect was mediated by whether the conversation partner was a friend or an individual unfamiliar to the speaker.&lt;/p&gt; &lt;p.small&gt;This research was supported by a Bilski-Mayer Fellowship. Scripting was primarily written in PsyScope with networking components developed using the ExBuilder experiment design toolkit. Eye-tracking data was analyzed using the &lt;tt&gt;saccades&lt;/tt&gt; and &lt;tt&gt;eyetrackingR&lt;/tt&gt; packages in R.&lt;/p&gt;</description>
        <pubDate>Sat, 09 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/cognitive%20science%20and%20psycholinguistics/2016/01/09/project-9/</link>
        <guid isPermaLink="true">https://jgutman.github.io/cognitive%20science%20and%20psycholinguistics/2016/01/09/project-9/</guid>
        
        
        <category>cognitive science and psycholinguistics</category>
        
      </item>
    
      <item>
        <title>French-English word alignment models</title>
        <description>&lt;p.small&gt;Many state-of-the-art machine translation systems consist of a language model and an alignment model trained on a large corpus of parallel texts with no word-to-word gloss available. In this project I implemented IBM alignment models 1 and 2 from scratch, trained on a parallel French-English corpus of Canadian parliament transcripts.&lt;/p&gt; &lt;p.small&gt;Expectation-maximization was used to train each alignment model, and the intersection of one-to-many alignments in both translation directions were taken to yield the best possible inferred alignment for each sentence. Cross-validation on model hyperparameters and detailed linguistic error analysis were conducted to optimize model performance.&lt;/p&gt; &lt;p.small&gt;Code for the alignment models and expectation-maximization algorithm was written in Java and run on a high-performance computing cluster. Performance of my model yielded the second-best alignment error rate out of all student-submitted models.&lt;/p&gt;</description>
        <pubDate>Fri, 08 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/machine%20learning%20and%20natural%20language%20processing/2016/01/08/project-8/</link>
        <guid isPermaLink="true">https://jgutman.github.io/machine%20learning%20and%20natural%20language%20processing/2016/01/08/project-8/</guid>
        
        
        <category>machine learning and natural language processing</category>
        
      </item>
    
      <item>
        <title>Morphosyntactic change in an online iterated learning paradigm</title>
        <description>&lt;p.small&gt;My thesis explored how we can model the functional constraints and cognitive biases that successive cohorts of learners impose on their linguistic input in order to make the language system more functionally communicative (i.e. language needs to be expressive, compositional, parsimonious, and learnable). &lt;/p&gt; &lt;p.small&gt;Using a Flash web applet shown to MTurk workers, learners engaged in an educational game to learn the vocabulary and grammar of an artificial language. Learners were tasked with producing utterances in the artificial language that would be subsequently provided to the following learning cohort as linguistic input. We looked at gradual changes in the morphosyntactic structure from the first to the last generation of each linguistic diffusion chain to determine if the cumulative changes imposed by learners demonstrated a tendency to reduce conditional entropy (i.e. unpredictable/context-independent variation), particularly in the word order and agent/theme case-marking system.&lt;/p&gt; &lt;p.small&gt;The key research question here was&amp;#58; do learners mutate their input in a manner that is consistent with a bias towards systematicity? Preliminary analyses suggested a bias towards reducing or eliminating unsystematic variation in linguistic structure not predictable from context. This experiment served as a proof-of-concept for the implementation of iterated artificial learning experiments in an online setting.&lt;/p&gt; &lt;p.small&gt;Code for this project was primarily written in Python. Audacity sound editing software, Poser video rendering and animation, and Amazon&#39;s Mechanical Turk were used to support the content creation and task automation used in the experiments.&lt;/p&gt;</description>
        <pubDate>Thu, 07 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/cognitive%20science%20and%20psycholinguistics/2016/01/07/project-7/</link>
        <guid isPermaLink="true">https://jgutman.github.io/cognitive%20science%20and%20psycholinguistics/2016/01/07/project-7/</guid>
        
        
        <category>cognitive science and psycholinguistics</category>
        
      </item>
    
      <item>
        <title>Metacognitive learning under adaptive help-seeking interventions</title>
        <description>&lt;p.small&gt;Cognitive tutoring agents can be extended with a help-seeking tutor module that tries to identify hint abuse and hint avoidance and provide corrective feedback. Here we extended previous work done by researchers at Carnegie Mellon University in collaboration with the Pittsburgh Science of Learning Center LearnLab in order to determine whether users interacting with this type of agent demonstrated improved metacognitive skills over time.&lt;/p&gt; &lt;p.small&gt;Metacognitive skill here is measured by the appropriateness of their resource and hint use conditional on their estimated knowledge of the mathematical concept being tested. In order to design the most effective interventions capable of delivering personalized adaptive instruction to students, we need to understand whether these metacognitive abilities are dissociable from the cognitive skills targeted by the tutoring system, and whether they are amenable to intervention.&lt;/p&gt; &lt;p.small&gt;We were particularly interested in whether these metacognitive learning curves revealed improvement in metacognitive skillfulness in a domain-general or context-specific manner. We found context-specific improvement in metacognitive resourcefulness and self-knowlege that showed limited generalization to new problem domains (or to new cognitive skills within the same broad domain).&lt;/p&gt;</description>
        <pubDate>Wed, 06 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/educational%20measurement%20and%20technology/2016/01/06/project-6/</link>
        <guid isPermaLink="true">https://jgutman.github.io/educational%20measurement%20and%20technology/2016/01/06/project-6/</guid>
        
        
        <category>educational measurement and technology</category>
        
      </item>
    
      <item>
        <title>Predicting the relative helpfulness of reading hint types</title>
        <description>&lt;p.small&gt;Using data from Project LISTEN, I built a model to predict the probability of a student reading a word successfully with no hesitation or disfluencies, given information about the student’s reading ability, prior reading experience, and the difficulty of the word attempted. In particular, I was interested in whether the specific type of help provided by the tutor during previous unsuccessful encounters with the word in question differentially affected the probability of success in the current encounter—are some kinds of hints more effective in certain contexts than others?&lt;/p&gt; &lt;p.small&gt;The children have the ability to request help, and the Reading Tutor can also provide help automatically. Because the Reading Tutor randomly chooses from up to 13 available help types, it creates a kind of natural experiment&amp;mdash;the best proxy we have to a true randomized controlled experiment.&lt;/p&gt; &lt;p.small&gt;Using pooled, unpooled, and hierarchical logistic regressions and classification tree models, we predict the probability of successful reading trials given word- and child-specific effects. Multiple approaches to feature selection including AIC-based stepwise methods were used to develop a model that accurately predicts reading difficulty, with unpooled estimates for each specific help type and shrinkage-adjusted estimates towards the common mean across all available help types.&lt;/p&gt; &lt;p.small&gt;Using these models, we can simulate an intervention where the type of help most likely to result in success for a particular child on a particular word is provided, instead of a randomly selected hint. I was able to demonstrate that while no overall differences in effectiveness between different kinds of hints were evident, some context-specific effects of type of help were non-zero, and selecting the optimal type of hint at each trial can potentially improve intervention effectiveness.&lt;/p&gt;&lt;p.small&gt;All models were developed in R, primarily using the &lt;tt&gt;glmnet&lt;/tt&gt;, &lt;tt&gt;caret&lt;/tt&gt;, and &lt;tt&gt;rpart&lt;/tt&gt; packages for classification trees, generalized linear models, and stepwise algorithms. Data obtained from Project LISTEN researchers at Carnegie Mellon University.&lt;/p&gt;</description>
        <pubDate>Tue, 05 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/educational%20measurement%20and%20technology/2016/01/05/project-5/</link>
        <guid isPermaLink="true">https://jgutman.github.io/educational%20measurement%20and%20technology/2016/01/05/project-5/</guid>
        
        
        <category>educational measurement and technology</category>
        
      </item>
    
      <item>
        <title>Collaborative problem solving and assessment</title>
        <description>&lt;p.small&gt;This is an ongoing project in collaboration with researchers at NYU Steinhardt&#39;s Center for the Promotion of Research Involving Innovative Statistical Methodology and the Computational Psychometrics Research Center of the Educational Testing Service. Our research addresses two fundamental questions in the nascent field of computer-supported collaborative learning and assessment:&lt;/p&gt; &lt;p.small&gt;First, can we determine the properties of assessment items that foster productive collaborations in an online/face-to-face partnered assessment setting?&lt;/p&gt; &lt;p.small&gt;Second, how can we use these assessment items to separately estimate an individual’s mathematical problem-solving ability from tasks completed in isolation as compared to the individual’s ability estimated from performance on tasks which encourage or require collaboration? Our approach differs from earlier CSCL research in that we focus on the assessment of their collaborative teamwork skills per se, but on measuring their cognitive ability within this collaborative and dynamic context.&lt;/p&gt; &lt;p.small&gt;In this project we are adapting established high school and college-level mathematics assessment items from the National Assessment of Educational Progress in order to elicit more collaborative and interdependent problem solving. These adapted items are used in implementing individual and collaborative assessment frameworks within OpenEdX courseware. We are working to develop a taxonomy of item characteristics associated with increased interdependence in domains with well-defined problems and solution paths, as is typical of mathematics.&lt;/p&gt; &lt;p.small&gt;Code for this project is primarily written in a combination of Python, XML, and Javascript. Materials are not yet publicly available, but for further information and background on the project, please see the links provided.&lt;/p&gt;</description>
        <pubDate>Mon, 04 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/educational%20measurement%20and%20technology/2016/01/04/project-4/</link>
        <guid isPermaLink="true">https://jgutman.github.io/educational%20measurement%20and%20technology/2016/01/04/project-4/</guid>
        
        
        <category>educational measurement and technology</category>
        
      </item>
    
      <item>
        <title>Supervised text classification in Reddit posts</title>
        <description>&lt;p.small&gt;We built a multi-class classifier for determining the optimal subreddit for a particular post given only its text. We were interested in comparing feature extraction methods based on standard bag-of-words approaches to distributed semantic representations constructed using skip-gram neural networks. Embeddings for particular words and Reddit posts as a whole were provided as features to linear classifiers (i.e. support vector machines) in order to predict optimal subreddit labels for each post. Bootstrap samples of the labeled input data were constructed to bias training towards higher quality posts, using proxy measures of post quality and relevance.&lt;/p&gt; &lt;p.small&gt;Models trained on traditional bag-of-words feature extraction approaches were compared to two main types of distributed meaning vector-space embedding approaches trained using single-layer neural networks&amp;#58; 1) the word embedding models&amp;mdash;which learned distinct representations for every word in the corpus vocabulary before aggregating these word vectors in a weighted or unweighted manner&amp;mdash; and 2) more complex document embedding neural net models that learned representations for both individual words and whole documents simultaneously.&lt;/p&gt; &lt;p.small&gt;Neither distributed neural network approach outperformed the simpler bag-of-words methods, but while these feature extraction methods did not substantially improve performance, they provided a reasonably efficient and scalable method of dimensionality reduction for text data without significant loss of information or decrease in performance.&lt;/p&gt;&lt;p.small&gt;The analyses and models used in this project were all written in Python using the scikit-learn library for machine learning and text processing, as well as the gensim and nltk for implementation of natural language and distributed models. Visualizations were constructed in R&#39;s ggplot2 as well as Python. Data obtained from Kaggle version of publicly available Reddit dataset.&lt;/p&gt;</description>
        <pubDate>Sun, 03 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/machine%20learning%20and%20natural%20language%20processing/2016/01/03/project-3/</link>
        <guid isPermaLink="true">https://jgutman.github.io/machine%20learning%20and%20natural%20language%20processing/2016/01/03/project-3/</guid>
        
        
        <category>machine learning and natural language processing</category>
        
      </item>
    
      <item>
        <title>Early warning system for hospital acquired infection rates</title>
        <description>&lt;p.small&gt;Hospitals with high levels of hospital-acquired infections risk losing Medicare funds and experiencing high rates of preventable patient mortality. Using autoregressive tree models and logistic regression, we developed a system for early identification of hospitals exceeding a threshold rate of central-line associated adverse events conditional on hospital characteristics. We incorporated a cost-sensitive risk function to bias the model towards increased sensitivity in identifying hospitals likely to exceed a threshold rate of hospital-acquired infections, controlling for patient characteristics using propensity score matching at the patient level.&lt;/p&gt; &lt;p.small&gt;To correct for prediction error arising from the rarity of the adverse events of interest relative to all opportunities for infection, we used a proxy target modeling approach. We developed a framework for evaluating the kinds of patient events that function as reliable target variables for predicting infection rates, and demonstrated the feasibility of proxy logistic regression in flagging at-risk hospitals prior to a spike in adverse patient events, in order to enable implementation and targeted enforcement of stricter infection prevention protocol and interventions.&lt;/p&gt; &lt;p.small&gt;The analyses and models used in this project were all written in Python using the scikit-learn library for machine learning and statistical modeling, as well as matplotlib for plotting and data visualization. De-identified open data were obtained directly from the Medicare.gov website. &lt;/p&gt;</description>
        <pubDate>Sat, 02 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/machine%20learning%20for%20social%20good/2016/01/02/project-2/</link>
        <guid isPermaLink="true">https://jgutman.github.io/machine%20learning%20for%20social%20good/2016/01/02/project-2/</guid>
        
        
        <category>machine learning for social good</category>
        
      </item>
    
      <item>
        <title>Anomaly detection in immigration court asylum decisions</title>
        <description>&lt;p.small&gt;Using data from 40 years of immigration court records in asylum decisions, we built an anomaly detection system to predict the probability of immigration judges granting asylum to families of asylum seekers given the characteristics of the refugee, case, and judge, and recent and immediate asylum decision history. We built predictive models using Adaboost, support vector machines, and other classification models to optimize prediction and recall across all judges. We were particularly interested in measuring streak avoidance behavior in judges’ decision-making and identifying judges whose behavior systematically deviated from expected voting patterns in asylum decisions involving particular refugee ethnicities and countries of origin. &lt;/p&gt; &lt;p.small&gt;Fairly extensive missing data necessitated use of multiple imputation in developing the candidate models for the analysis. A hierachical non-parametric model was developed using families nested in judges nested in courts nested in districts. A complete lack of variation at the within-family level motivated the use of families of asylum-seekers as the smallest unit of analysis, rather than individuals. Adaboost and linear support vector machines yielded the lowest mean error across all classes of models when optimizing for the F1-measure on asylum grants.&lt;/p&gt; &lt;p.small&gt;In refining these analyses we developed a technique for predicting the expected behavior of an individual judge from the collective decision-making behavior of all other judges, using the multilevel structure to weight the prediction more heavily towards judges closer to the target judge in the hierarchy. We used these weighted predictions to flag judges whose asylum granting decisions did not conform to the &quot;collective wisdom&quot; prediction of the system as a whole. A key finding in our analysis was that decision records which preceded the target case by over a year showed almost no interdependence either within or across judges, as the factors which contribute to reasonable decision-making behavior in asylum cases typically changed too drastically within a year to be informative for time lags greater than one year.&lt;/p&gt; &lt;p.small&gt;The analyses and models used in this project were all written in Python using the scikit-learn library for machine learning and statistical modeling. Sensitive and confidential datasets provided by researchers at the National Bureau of Economic Research. Additional features merged from open data obtained from the U.N. High Commissioner for Refugees and World Bank databases.&lt;/p&gt;</description>
        <pubDate>Fri, 01 Jan 2016 00:00:00 -0500</pubDate>
        <link>https://jgutman.github.io/machine%20learning%20for%20social%20good/2016/01/01/project-1/</link>
        <guid isPermaLink="true">https://jgutman.github.io/machine%20learning%20for%20social%20good/2016/01/01/project-1/</guid>
        
        
        <category>machine learning for social good</category>
        
      </item>
    
  </channel>
</rss>
